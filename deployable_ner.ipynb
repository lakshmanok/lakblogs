{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deployable_ner.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "QXpfXP8coO9r"
      ],
      "authorship_tag": "ABX9TyMhNO/eTS4tw26obJrFS2ES"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to write a Custom Keras Model so that it can be deployed for Serving\n",
        "<i> Accompanies this blog post https://medium.com/@lakshmanok/how-to-write-a-custom-keras-model-so-that-it-can-be-deployed-for-serving-7d81ace4a1f8 </i>\n",
        "\n",
        "At some point, you will find yourself going beyond pre-built Keras capabilities and subclassing Keras layers and models.\n",
        "\n",
        "Unfortunately, when you do that, exporting the model so that it can be easily deployed becomes quite difficult.\n",
        "\n",
        "The documentation that explains what you have do is quite scattered. My aim with this notebook is to show you an end-to-end example of how you can solve this problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "bB6CYeQ1Q2Xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER Transformer model\n",
        "\n",
        "To explain, I will take this NER transformer model that is one of the official Keras examples:\n",
        "\n",
        "https://keras.io/examples/nlp/ner_transformers/\n",
        "\n",
        "Basically, an NER model identifies entities. Suppose we have a NER model that is trained to identify names and locations. Then, it will take a sentence of the form:\n",
        "\n",
        "<i> John went to Paris </i>\n",
        "\n",
        "and return:\n",
        "\n",
        "<i> NAME out out LOCATION </i>\n",
        "\n",
        "How this model works itself isn't all that important.\n",
        "Just that it involves custom Keras layers and a custom Keras model"
      ],
      "metadata": {
        "id": "X3wfsoXkYMpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from collections import Counter\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                keras.layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        maxlen = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "        token_embeddings = self.token_emb(inputs)\n",
        "        return token_embeddings + position_embeddings\n",
        "\n",
        "class NERModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
        "    ):\n",
        "        super(NERModel, self).__init__()\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.transformer_block(x)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.ff(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.ff_final(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3-WkNJ2tXj6s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic dataset\n",
        "\n",
        "The training dataset for a NER model is in \"[IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))\" format.\n",
        "* I=Inside\n",
        "* OUT=Outside\n",
        "* B=Beginnning\n",
        "\n",
        "It's pretty intuitive with an example.\n",
        "In our case, the description will be tokenized as follows, assuming that we are interested only in the firm and the customer id:\n",
        "<pre>\n",
        "Jean   B-NAME \n",
        "Baptiste  I-NAME\n",
        "Chevalier I-NAME\n",
        "went OUT\n",
        "to   OUT\n",
        "Paris  B-LOCATION\n",
        "France  I-LOCATION\n",
        "</pre>\n",
        "\n",
        "Let's create a quick synthetic dataset to train the model on"
      ],
      "metadata": {
        "id": "QXpfXP8coO9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "names = [\n",
        "         'Emmanuel Macaron', 'Xi Jinping', 'Joe Biden',\n",
        "         'Vladimir Putin', 'Vlodymyr Zelenskyy', 'Narendra Modi', \n",
        "         'Justin Trudeau', 'Jair Bolsonaro']\n",
        "junk_words = ['went to', 'visited', 'was in', 'was chased out of']\n",
        "locations = ['Paris France', 'Beijing China', 'Washington DC',\n",
        "             'Moscow Russia', 'Kyiv Ukraine', 'New Delhi India',\n",
        "             'Ottawa Canada', 'Brasilia Brazil']\n",
        "\n",
        "def generate_description():\n",
        "  descr = []\n",
        "  labels = []\n",
        "  \n",
        "  # name\n",
        "  gen = random.choice(names).split()\n",
        "  descr += gen\n",
        "  labels += ['B-NAME'] + ['I-NAME'] * (len(gen) - 1)\n",
        "\n",
        "  # verb\n",
        "  gen = random.choice(junk_words).split()\n",
        "  descr += gen\n",
        "  labels += ['OUT'] * len(gen)\n",
        "\n",
        "  # place\n",
        "  gen = random.choice(locations).split()\n",
        "  descr += gen\n",
        "  labels += ['B-LOCATION'] + ['I-LOCATION'] * (len(gen) - 1)\n",
        "\n",
        "  return ' '.join(descr), ' '.join(labels)\n",
        "\n",
        "generate_description()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUyhEcy_oX2X",
        "outputId": "26828886-4d81-4cd3-ab6e-f4749a0c89e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Emmanuel Macaron went to Paris France',\n",
              " 'B-NAME I-NAME OUT OUT B-LOCATION I-LOCATION')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame(data=[generate_description() for x in range(1000)], columns=['description', 'labels'])\n",
        "valid_df = pd.DataFrame(data=[generate_description() for x in range(100)], columns=['description', 'labels'])\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ELsz1oubr9f2",
        "outputId": "af00a13e-8773-4cd6-f557-8fc41904611f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      description  \\\n",
              "0         Vlodynyr Zelenskyy was in Ottawa Canada   \n",
              "1  Vladimir Putin was chased out of Washington DC   \n",
              "2         Emmanuel Macaron was in New Delhi India   \n",
              "3      Vlodynyr Zelenskyy visited New Delhi India   \n",
              "4          Vlodynyr Zelenskyy was in Paris France   \n",
              "\n",
              "                                              labels  \n",
              "0        B-NAME I-NAME OUT OUT B-LOCATION I-LOCATION  \n",
              "1  B-NAME I-NAME OUT OUT OUT OUT B-LOCATION I-LOC...  \n",
              "2  B-NAME I-NAME OUT OUT B-LOCATION I-LOCATION I-...  \n",
              "3  B-NAME I-NAME OUT B-LOCATION I-LOCATION I-LOCA...  \n",
              "4        B-NAME I-NAME OUT OUT B-LOCATION I-LOCATION  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ed080ea-a332-49a1-8f90-04b4100cf8dc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Vlodynyr Zelenskyy was in Ottawa Canada</td>\n",
              "      <td>B-NAME I-NAME OUT OUT B-LOCATION I-LOCATION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vladimir Putin was chased out of Washington DC</td>\n",
              "      <td>B-NAME I-NAME OUT OUT OUT OUT B-LOCATION I-LOC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Emmanuel Macaron was in New Delhi India</td>\n",
              "      <td>B-NAME I-NAME OUT OUT B-LOCATION I-LOCATION I-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Vlodynyr Zelenskyy visited New Delhi India</td>\n",
              "      <td>B-NAME I-NAME OUT B-LOCATION I-LOCATION I-LOCA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Vlodynyr Zelenskyy was in Paris France</td>\n",
              "      <td>B-NAME I-NAME OUT OUT B-LOCATION I-LOCATION</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ed080ea-a332-49a1-8f90-04b4100cf8dc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ed080ea-a332-49a1-8f90-04b4100cf8dc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ed080ea-a332-49a1-8f90-04b4100cf8dc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv('train.csv', index=False, header=False, sep='\\t')\n",
        "valid_df.to_csv('valid.csv', index=False, header=False, sep='t')"
      ],
      "metadata": {
        "id": "_9ukZPjUveVW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aIUAByWwJTz",
        "outputId": "3dee96a6-38a8-4c4a-b687-ce26c3e820fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vlodynyr Zelenskyy was in Ottawa Canada\tB-NAME I-NAME OUT OUT B-LOCATION I-LOCATION\n",
            "Vladimir Putin was chased out of Washington DC\tB-NAME I-NAME OUT OUT OUT OUT B-LOCATION I-LOCATION\n",
            "Emmanuel Macaron was in New Delhi India\tB-NAME I-NAME OUT OUT B-LOCATION I-LOCATION I-LOCATION\n",
            "Vlodynyr Zelenskyy visited New Delhi India\tB-NAME I-NAME OUT B-LOCATION I-LOCATION I-LOCATION\n",
            "Vlodynyr Zelenskyy was in Paris France\tB-NAME I-NAME OUT OUT B-LOCATION I-LOCATION\n",
            "Vlodynyr Zelenskyy went to Kyiv Ukraine\tB-NAME I-NAME OUT OUT B-LOCATION I-LOCATION\n",
            "Justin Trudeau was in Moscow Russia\tB-NAME I-NAME OUT OUT B-LOCATION I-LOCATION\n",
            "Justin Trudeau was chased out of Kyiv Ukraine\tB-NAME I-NAME OUT OUT OUT OUT B-LOCATION I-LOCATION\n",
            "Vladimir Putin visited New Delhi India\tB-NAME I-NAME OUT B-LOCATION I-LOCATION I-LOCATION\n",
            "Emmanuel Macaron was in Brasilia Brazil\tB-NAME I-NAME OUT OUT B-LOCATION I-LOCATION\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the NER Model\n",
        "\n",
        "Again, some of the details here will become important, but for now, let's just say that we write a tf.data input pipeline to read the data and pass it into the model for training."
      ],
      "metadata": {
        "id": "FuqHrykXnAKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The labels that we are interested in\n",
        "NER_LABELS = [\"NAME\", \"LOCATION\"]\n",
        "MAX_LEN = 16  # number of words in description\n",
        "\n",
        "# How many times does a word have to appear in the training dataset before\n",
        "# we treat it as fixed?\n",
        "MIN_FREQ = 5\n",
        "EMBED_DIM = 8\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "EXPORT_PATH = \"ner_model\""
      ],
      "metadata": {
        "id": "bZejpsg2nXff"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab_lookup(filename):\n",
        "  train_df = pd.read_csv(filename, sep='\\t', names=['descr', 'labels'])\n",
        "  all_tokens = sum(train_df[\"descr\"].apply(str.split), [])\n",
        "  print(all_tokens[:5])\n",
        "  all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
        "  print(all_tokens_array[:5])\n",
        "\n",
        "  # all the unique customer ids etc. should not be included in vocabulary\n",
        "  counter = Counter(all_tokens_array)\n",
        "  vocabulary = [elem for elem, cnt in counter.items() if cnt >= MIN_FREQ]\n",
        "  vocabulary += [ '[PAD]' ]\n",
        "  print('{} unique tokens in training file; {} occur more than {}x'.format(\n",
        "      len(counter), len(vocabulary), MIN_FREQ))\n",
        "\n",
        "  # vocabulary size is the number of words + the PAD\n",
        "  # The StringLookup class will convert tokens to token IDs\n",
        "  return len(vocabulary) + 1, keras.layers.StringLookup(vocabulary=vocabulary)\n",
        "\n",
        "vocab_size, vocab_lookup_layer = create_vocab_lookup('train.csv')\n",
        "print(vocab_lookup_layer(['notinvocab', 'paris', 'modi', 'xi']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha3fyPBE04XW",
        "outputId": "46feaea7-0e80-4d84-c1be-e7a6e0ebba81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Vlodynyr', 'Zelenskyy', 'was', 'in', 'Ottawa']\n",
            "['vlodynyr' 'zelenskyy' 'was' 'in' 'ottawa']\n",
            "41 unique tokens in training file; 42 occur more than 5x\n",
            "tf.Tensor([ 0 20 33 36], shape=(4,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tag_lookup_table():\n",
        "    iob_labels = [\"B\", \"I\"]\n",
        "    all_labels = [(label1, label2) for label2 in NER_LABELS for label1 in iob_labels]\n",
        "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
        "    all_labels = [\"[PAD]\", \"OUT\"] + all_labels\n",
        "    return all_labels, dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
        "\n",
        "all_labels, mapping = make_tag_lookup_table()\n",
        "num_tags = len(all_labels)\n",
        "print(mapping)\n",
        "label_lookup_layer = keras.layers.StringLookup(vocabulary=all_labels, oov_token='[PAD]')\n",
        "print(label_lookup_layer(['[PAD]', 'OUT', 'I-FIRM']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8KONQB_3mHn",
        "outputId": "1232210f-28d2-4375-a813-03bd0066c65a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '[PAD]', 1: 'OUT', 2: 'B-NAME', 3: 'I-NAME', 4: 'B-LOCATION', 5: 'I-LOCATION'}\n",
            "tf.Tensor([0 1 0], shape=(3,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_record_to_training_data(record):\n",
        "    record = tf.strings.split(record, sep=\"\\t\")\n",
        "\n",
        "    tokens = tf.strings.split(record[0])\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    tokens = vocab_lookup_layer(tokens)\n",
        "\n",
        "    tags = tf.strings.split(record[1])\n",
        "    tags = label_lookup_layer(tags)\n",
        "    return tokens, tags\n",
        "\n",
        "# We use `padded_batch` here because each record in the dataset has a\n",
        "# different length.\n",
        "batch_size = 32\n",
        "train_dataset = (\n",
        "    tf.data.TextLineDataset('train.csv')\n",
        "    .map(map_record_to_training_data)\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "val_dataset = (\n",
        "    tf.data.TextLineDataset('valid.csv')\n",
        "    .map(map_record_to_training_data)\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "\n",
        "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
        "\n",
        "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
        "    def __init__(self, name=\"custom_ner_loss\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
        "        )\n",
        "        loss = loss_fn(y_true, y_pred)\n",
        "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
        "        loss = loss * mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "loss = CustomNonPaddingTokenLoss()\n",
        "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
        "ner_model.fit(train_dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0r8xKA6cnTu",
        "outputId": "17e188e0-cc57-482c-af20-94737bbccf4e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 4s 20ms/step - loss: 0.7741\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 0.0569\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.0135\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.0073\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.0051\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 0.0037\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 0.0028\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0021\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.0018\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.0014\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7febd887bf50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export model and deploy the model\n",
        "\n",
        "Here's where the interesting things start.\n",
        "Let's try to save the model so that we can deploy it.\n",
        "\n",
        "Typically, this is what it will involve:\n",
        "<pre>\n",
        "model.save(EXPORT_PATH)\n",
        "</pre>\n",
        "Then, we will take the saved model and give to a service such as Sagemaker or Vertex AI and it will do:\n",
        "<pre>\n",
        "model = saved_model.load_model(EXPORT_PATH)\n",
        "model.predict(...)\n",
        "</pre>\n",
        "Unfortunately, because of all the custom layers and code above, this stratightforward approach won't work.\n",
        "\n",
        "Let's see:"
      ],
      "metadata": {
        "id": "QbM6sBWmeZgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {EXPORT_PATH}\n",
        "ner_model.save(EXPORT_PATH)\n",
        "!ls -l {EXPORT_PATH}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqlhhjHOe-ht",
        "outputId": "41f86bde-9d87-4674-f02e-bb30f1eafa58"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_2_layer_call_fn, embedding_2_layer_call_and_return_conditional_losses, embedding_3_layer_call_fn, embedding_3_layer_call_and_return_conditional_losses, multi_head_attention_1_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ner_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ner_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 544\n",
            "drwxr-xr-x 2 root root   4096 Apr 15 04:30 assets\n",
            "-rw-r--r-- 1 root root  19920 Apr 15 04:30 keras_metadata.pb\n",
            "-rw-r--r-- 1 root root 527307 Apr 15 04:30 saved_model.pb\n",
            "drwxr-xr-x 2 root root   4096 Apr 15 04:30 variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(EXPORT_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "rs2Z4MIrfE2k",
        "outputId": "e1f4ba93-78cb-4215-c07f-cff781b70d78"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ac6548d34dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPORT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m sample_input = [\n\u001b[1;32m      3\u001b[0m      \u001b[0;34m\"Justin Trudeau went to New Delhi India\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0;34m\"Total Gaz Electricité De Grenoble charges client 2127\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    561\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     raise ValueError(\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0;34mf'Unknown {printable_module_name}: {class_name}. Please ensure this '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;34m'object is passed to the `custom_objects` argument. See '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;34m'https://www.tensorflow.org/guide/keras/save_and_serialize'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown loss function: CustomNonPaddingTokenLoss. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Unknown loss function\n",
        "\n",
        "The error above:\n",
        "<pre>\n",
        "Unknown loss function: CustomNonPaddingTokenLoss. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
        "</pre>\n",
        "\n",
        "How do we solve this? I will show you how to register custom objects later in this notebook. But the first thing to realize is that WE DO NOT NEED TO EXPORT THE LOSS. The loss is needed only for training, not for deployment.\n",
        "\n",
        "So, we have a much simpler thing we can do. Just remove the loss."
      ],
      "metadata": {
        "id": "38t9tpoffiXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {EXPORT_PATH}\n",
        "# remove the custom loss before saving.\n",
        "temp_model = ner_model\n",
        "temp_model.compile('adam', loss=None)\n",
        "temp_model.save(EXPORT_PATH)\n",
        "!ls -l {EXPORT_PATH}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEbAu-E6ffZi",
        "outputId": "45a5702b-031b-4923-d0ad-fea0d56d2514"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_2_layer_call_fn, embedding_2_layer_call_and_return_conditional_losses, embedding_3_layer_call_fn, embedding_3_layer_call_and_return_conditional_losses, multi_head_attention_1_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ner_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ner_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 468\n",
            "drwxr-xr-x 2 root root   4096 Apr 15 04:34 assets\n",
            "-rw-r--r-- 1 root root  19564 Apr 15 04:34 keras_metadata.pb\n",
            "-rw-r--r-- 1 root root 448230 Apr 15 04:34 saved_model.pb\n",
            "drwxr-xr-x 2 root root   4096 Apr 15 04:34 variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(EXPORT_PATH)"
      ],
      "metadata": {
        "id": "vNS93NWzfeOs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Success!   Bottom line: remove custom losses before exporting Keras models for deployment."
      ],
      "metadata": {
        "id": "AkrpAVFCgOMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Wrong input shape\n",
        "\n",
        "Now, let's do what TensorFlow Serving (or the managed service that wraps TensorFlow Serving, such as Sagemaker or Keras) does: call the predict() method of the model we just loaded."
      ],
      "metadata": {
        "id": "ESHVETNCgYzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input = [\n",
        "     \"Justin Trudeau went to New Delhi India\",\n",
        "     \"Vladimir Putin was chased out of Kyiv Ukraine\"\n",
        "]\n",
        "model.predict(sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QjzTdeSXgth_",
        "outputId": "b89b5611-87db-4b88-a2fe-165f095c2131"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d8689ca03c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m      \u001b[0;34m\"Vladimir Putin was chased out of Kyiv Ukraine\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ]\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"ner_model_1\" (type NERModel).\n    \n    Could not find matching concrete function to call loaded from the SavedModel. Got:\n      Positional arguments (2 total):\n        * Tensor(\"inputs:0\", shape=(None,), dtype=string)\n        * False\n      Keyword arguments: {}\n    \n     Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (2 total):\n        * TensorSpec(shape=(None, None), dtype=tf.int64, name='inputs')\n        * False\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (2 total):\n        * TensorSpec(shape=(None, None), dtype=tf.int64, name='inputs')\n        * True\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (2 total):\n        * TensorSpec(shape=(None, None), dtype=tf.int64, name='input_1')\n        * False\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (2 total):\n        * TensorSpec(shape=(None, None), dtype=tf.int64, name='input_1')\n        * True\n      Keyword arguments: {}\n    \n    Call arguments received:\n      • args=('tf.Tensor(shape=(None,), dtype=string)',)\n      • kwargs={'training': 'False'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Capturing the preprocessing\n",
        "\n",
        "The error above is:\n",
        "<pre>\n",
        "ValueError: Exception encountered when calling layer \"ner_model_1\" (type NERModel).\n",
        "    \n",
        "    Could not find matching concrete function to call loaded from the SavedModel. Got:\n",
        "      Positional arguments (2 total):\n",
        "        * Tensor(\"inputs:0\", shape=(None,), dtype=string)\n",
        "        * False\n",
        "      Keyword arguments: {}\n",
        "    \n",
        "     Expected these arguments to match one of the following 4 option(s):\n",
        "    \n",
        "    Option 1:\n",
        "      Positional arguments (2 total):\n",
        "        * TensorSpec(shape=(None, None), dtype=tf.int64, name='inputs')\n",
        "        * False\n",
        "      Keyword arguments: {}\n",
        "</pre>\n",
        "\n",
        "Essentially, we are trying to send in a full sentence, but our model was trained on a set of vocabulary ids. That's why the expected input is a set of integers.\n",
        "\n",
        "We did this in our tf.data() pipeline when we called tf.strings.split(), tf.strings.lower and vocab_lookup_layer()\n",
        "<pre>\n",
        "def map_record_to_training_data(record):\n",
        "    record = tf.strings.split(record, sep=\"\\t\")\n",
        "\n",
        "    tokens = tf.strings.split(record[0])\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    tokens = vocab_lookup_layer(tokens)\n",
        "\n",
        "    tags = tf.strings.split(record[1])\n",
        "    tags = label_lookup_layer(tags)\n",
        "    return tokens, tags\n",
        "</pre>\n",
        "\n",
        "We've got to do that.\n",
        "\n",
        "How?\n",
        "\n",
        "Well, we could use a [preprocessing container](https://cloud.google.com/blog/topics/developers-practitioners/add-preprocessing-functions-tensorflow-models-and-deploy-vertex-ai) on Vertex AI or the similar functionality on Sagemaker. But that sort of defeats our purpose of having a simple, all-in deployed Keras model.\n",
        "\n",
        "Instead, reorganize our tf.data input pipeline. What we want is to have a function (here, I call it process_descr) that we can can call from both the tf.data pipeline and from our exported model:"
      ],
      "metadata": {
        "id": "LmQAG39GhChm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_descr(descr):\n",
        "  # split the string on spaces, and make it a rectangular tensor\n",
        "  tokens = tf.strings.split(tf.strings.lower(descr))\n",
        "  tokens = vocab_lookup_layer(tokens)\n",
        "  max_len = MAX_LEN # max([x.shape[0] for x in tokens])\n",
        "  input_words = tokens.to_tensor(default_value=0, shape=[tf.rank(tokens), max_len])\n",
        "  return input_words\n",
        "\n",
        "def process_tag(tags):\n",
        "  # split the string on spaces, and make it a rectangular tensor\n",
        "  tags = label_lookup_layer(tf.strings.split(tags))\n",
        "  max_len = MAX_LEN # max([x.shape[0] for x in tags])\n",
        "  tags = tags.to_tensor(default_value=0, shape=[tf.rank(tags), max_len])\n",
        "  return tags\n",
        "\n",
        "def map_record_to_training_data(record):\n",
        "    record = tf.strings.split(record, sep=\"\\t\")\n",
        "    return record[0], record[1]\n",
        "\n",
        "def read_dataset(filename, batch_size=BATCH_SIZE):\n",
        "  data = tf.data.TextLineDataset(filename)\n",
        "  dataset = (\n",
        "      data\n",
        "      .map(map_record_to_training_data)\n",
        "      .padded_batch(batch_size)\n",
        "      .map(lambda d, t: (process_descr(d), process_tag(t)))\n",
        "  )\n",
        "  return dataset\n",
        "\n",
        "print(list(read_dataset('train.csv', 3).take(1).as_numpy_iterator()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta9NbD73wKzj",
        "outputId": "804bd225-533a-44ca-f3d7-ee91ee69dc46"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(array([[ 1,  2,  3,  4,  5,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "       [ 7,  8,  3,  9, 10, 11, 12, 13,  0,  0,  0,  0,  0,  0,  0,  0]]), array([[2, 3, 1, 1, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [2, 3, 1, 1, 1, 1, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0]]))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I should also come clean here. It's not exactly identical. There was a bit of Python list comprehension:\n",
        "<pre>\n",
        "max([x.shape[0] for x in tokens])\n",
        "</pre>\n",
        "The equivalent TensorFlow code would be:\n",
        "<pre>\n",
        "tf.reduce_max(tf.map_fn(lambda x: x.shape[0]))\n",
        "</pre>\n",
        "However, this function is not traceable because the batch size is not known (long story). I replaced it by MAX_LEN. A little inefficient since all batches are being padded to the MAX_LEN even if the batch contains only shorter strings."
      ],
      "metadata": {
        "id": "JFr9-Bh1o_ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model\n",
        "\n",
        "Let's see that we can train the model just like before (it's the same code, except that all the preprocessing code is in process_descr):"
      ],
      "metadata": {
        "id": "kUPUKRLQc1Lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = read_dataset('train.csv')\n",
        "val_dataset = read_dataset('valid.csv')\n",
        "ner_model = NERModel(num_tags, vocab_size,\n",
        "                     maxlen=MAX_LEN, embed_dim=EMBED_DIM)\n",
        "ner_model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy')\n",
        "ner_model.fit(train_dataset, epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "gZ6Ss-iH0ZYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca71d3f-d3a4-4275-f769-e34f753809e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 3s 11ms/step - loss: 0.9336\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.5821\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.4304\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3225\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.2270\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.1517\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.1088\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0810\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0610\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0468\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7febd4ba2350>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a new model with preproocessing layer\n",
        "\n",
        "Now, when we save the model, we have to ensure that preprocessing method gets called. One way to do that is to define a prediction signature that calls the preprocessing function.  However, that is problematic because if there are errors in your custom model, you won't get the errors (ask me how I know).\n",
        "\n",
        "A simpler approach that tells you the errors and gives you a chance to fix them is to do what we did with the loss function. Define a new standard model that has a lambda layer that does the preprocessing before feeding it to the custom model and write that out."
      ],
      "metadata": {
        "id": "qtmp0G1ccN1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_model = tf.keras.Sequential([\n",
        "  tf.keras.Input(shape=[], dtype=tf.string, name='description'),\n",
        "  tf.keras.layers.Lambda(process_descr),\n",
        "  ner_model                            \n",
        "])\n",
        "temp_model.compile('adam', loss=None)\n",
        "temp_model.save(EXPORT_PATH)\n",
        "!ls -l {EXPORT_PATH}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "HEOsoM71rKdk",
        "outputId": "31ec5522-e2c1-4e87-a534-54a165f031ce"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_4_layer_call_fn, embedding_4_layer_call_and_return_conditional_losses, embedding_5_layer_call_fn, embedding_5_layer_call_and_return_conditional_losses, multi_head_attention_2_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-58e42f73e92c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      6\u001b[0m \u001b[0mtemp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtemp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPORT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls -l {EXPORT_PATH}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_map_captures_to_created_tensors\u001b[0;34m(original_captures, resource_map)\u001b[0m\n\u001b[1;32m    532\u001b[0m           \u001b[0;34m\"directly.\\n\\n Trackable Python objects referring to this tensor \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m           \"(from gc.get_referrers, limited to two hops):\\n{}\".format(\"\\n\".join(\n\u001b[0;32m--> 534\u001b[0;31m               [repr(obj) for obj in trackable_referrers])))\n\u001b[0m\u001b[1;32m    535\u001b[0m     \u001b[0mexport_captures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexport_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Tried to export a function which references 'untracked' resource Tensor(\"32406:0\", shape=(), dtype=resource). TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\n\n Trackable Python objects referring to this tensor (from gc.get_referrers, limited to two hops):\n<tensorflow.python.ops.lookup_ops.StaticHashTable object at 0x7febda2a8890>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3: Untracked Tensor\n",
        "\n",
        "The error message above is:\n",
        "<pre>\n",
        "Tried to export a function which references 'untracked' resource Tensor\n",
        "</pre>\n",
        "\n",
        "What's that about? Here's the issue:\n",
        "\n",
        "When you write a custom Keras layer or Keras loss or Keras model, you are defining code. But when you are exporting the model, you have to make a flat file out of it. What happens to the code? It's lost! How can the prediction work then?\n",
        "\n",
        "You need to tell Keras how to pass in all the constructor arguments etc.\n",
        "\n",
        "The way you do that is by defining a getConfig() method that has all the constructor arguments. Basically, a custom layer that looks like this:\n",
        "<pre>\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        maxlen = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "        token_embeddings = self.token_emb(inputs)\n",
        "        return token_embeddings + position_embeddings\n",
        "</pre>\n",
        "\n",
        "will have to look like this:\n",
        "<pre>\n",
        "@tf.keras.utils.register_keras_serializable() # 1\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs): # 2\n",
        "        super(TokenAndPositionEmbedding, self).__init__(**kwargs) # 3\n",
        "        self.token_emb = keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        #4 save the constructor parameters for get_config()\n",
        "        self.maxlen = maxlen\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        maxlen = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "        token_embeddings = self.token_emb(inputs)\n",
        "        return token_embeddings + position_embeddings\n",
        "\n",
        "    def get_config(self): # 5\n",
        "        config = super().get_config()\n",
        "        # save constructor args\n",
        "        config['maxlen'] = self.maxlen\n",
        "        config['vocab_size'] = self.vocab_size\n",
        "        config['embed_dim'] = self.embed_dim\n",
        "        return config\n",
        "</pre>\n",
        "\n",
        "There are 5 changes:\n",
        "1. Add the annotation to register the custom layer with Keras\n",
        "2. Add a **kwargs to the constructor parameter\n",
        "3. Add a **kwargs to the super constructor\n",
        "4. Save the constructor parameters as instance fields\n",
        "5. Define a get_config method that saves the constructor args\n",
        "\n",
        "Let's do them to our custom Layer and Model classes."
      ],
      "metadata": {
        "id": "u1w8aChiriYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from collections import Counter\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable() # 1\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs): #2\n",
        "        super(TransformerBlock, self).__init__(**kwargs) #3\n",
        "\n",
        "        #4 save the constructor parameters for get_config() to work properly\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "        self.att = keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                keras.layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self): #5\n",
        "        config = super().get_config()\n",
        "        # save constructor args\n",
        "        config['embed_dim'] = self.embed_dim\n",
        "        config['num_heads'] = self.num_heads\n",
        "        config['ff_dim'] = self.ff_dim\n",
        "        config['rate'] = self.rate\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable() # 1\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs): #2\n",
        "        super(TokenAndPositionEmbedding, self).__init__(**kwargs) #3\n",
        "\n",
        "        #4 save the constructor parameters for get_config()\n",
        "        self.maxlen = maxlen\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim    \n",
        "\n",
        "        self.token_emb = keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        maxlen = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "        token_embeddings = self.token_emb(inputs)\n",
        "        return token_embeddings + position_embeddings\n",
        "\n",
        "    def get_config(self): #5\n",
        "        config = super().get_config()\n",
        "        # save constructor args\n",
        "        config['maxlen'] = self.maxlen\n",
        "        config['vocab_size'] = self.vocab_size\n",
        "        config['embed_dim'] = self.embed_dim\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable() # 1\n",
        "class NERModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32, **kwargs #2\n",
        "    ):\n",
        "        super(NERModel, self).__init__(**kwargs) #3\n",
        "\n",
        "        #4 save the constructor parameters for get_config()\n",
        "        self.num_tags = num_tags\n",
        "        self.vocab_size = vocab_size\n",
        "        self.maxlen = maxlen\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.transformer_block(x)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.ff(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.ff_final(x)\n",
        "        return x\n",
        "\n",
        "    def get_config(self): #5\n",
        "        config = super().get_config()\n",
        "        # save constructor args\n",
        "        config['num_tags'] = self.num_tags \n",
        "        config['vocab_size'] = self.vocab_size\n",
        "        config['maxlen'] = self.maxlen\n",
        "        config['embed_dim'] = self.embed_dim\n",
        "        config['num_heads'] = self.num_heads\n",
        "        config['ff_dim'] = self.ff_dim\n",
        "        return config"
      ],
      "metadata": {
        "id": "5n6j3kyasS56"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = read_dataset('train.csv')\n",
        "val_dataset = read_dataset('valid.csv')\n",
        "ner_model = NERModel(num_tags, vocab_size,\n",
        "                     maxlen=MAX_LEN, embed_dim=EMBED_DIM)\n",
        "ner_model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy')\n",
        "ner_model.fit(train_dataset, epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDcpVlBZvLi5",
        "outputId": "6915ca89-4962-48b5-dce9-8cb7609201d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "32/32 [==============================] - 4s 12ms/step - loss: 1.2600\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6781\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4576\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3212\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2233\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.1420\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.1016\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0722\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0489\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0404\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7febd8823d90>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_model = tf.keras.Sequential([\n",
        "  tf.keras.Input(shape=[], dtype=tf.string, name='description'),\n",
        "  tf.keras.layers.Lambda(process_descr),\n",
        "  ner_model                            \n",
        "])\n",
        "temp_model.compile('adam', loss=None)\n",
        "temp_model.save(EXPORT_PATH)\n",
        "!ls -l {EXPORT_PATH}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "lgc03mrRvSmo",
        "outputId": "bd3e6b68-1250-4c6e-8c5d-85a4264e21ca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_6_layer_call_fn, embedding_6_layer_call_and_return_conditional_losses, embedding_7_layer_call_fn, embedding_7_layer_call_and_return_conditional_losses, multi_head_attention_3_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-58e42f73e92c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      6\u001b[0m \u001b[0mtemp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtemp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPORT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls -l {EXPORT_PATH}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_map_captures_to_created_tensors\u001b[0;34m(original_captures, resource_map)\u001b[0m\n\u001b[1;32m    532\u001b[0m           \u001b[0;34m\"directly.\\n\\n Trackable Python objects referring to this tensor \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m           \"(from gc.get_referrers, limited to two hops):\\n{}\".format(\"\\n\".join(\n\u001b[0;32m--> 534\u001b[0;31m               [repr(obj) for obj in trackable_referrers])))\n\u001b[0m\u001b[1;32m    535\u001b[0m     \u001b[0mexport_captures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexport_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Tried to export a function which references 'untracked' resource Tensor(\"39565:0\", shape=(), dtype=resource). TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\n\n Trackable Python objects referring to this tensor (from gc.get_referrers, limited to two hops):\n<tensorflow.python.ops.lookup_ops.StaticHashTable object at 0x7febda2a8890>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4: Untracked resource in Lambda Layer\n",
        "\n",
        "We still have a problem. Even though we went through and fixed all the custom layers and models, there is still one piece of user-defined code.\n",
        "\n",
        "The Lamda layer we are using for the preprocessing! It uses the vocabulary, and that vocab_lookup_layer is a resource that is untracked:\n",
        "<pre>\n",
        "def process_descr(descr):\n",
        "  # split the string on spaces, and make it a rectangular tensor\n",
        "  tokens = tf.strings.split(tf.strings.lower(descr))\n",
        "  tokens = vocab_lookup_layer(tokens)\n",
        "  max_len = MAX_LEN # max([x.shape[0] for x in tokens])\n",
        "  input_words = tokens.to_tensor(default_value=0, shape=[tf.rank(tokens), max_len])\n",
        "  return input_words\n",
        "</pre>\n",
        "\n",
        "Bottom line: Lambda layers are dangerous and it's difficult to realize what resources we are forgetting.\n",
        "\n",
        "I recommend that you get rid of any Lambda layers and replace them by custom layers.\n",
        "\n",
        "Let's do that."
      ],
      "metadata": {
        "id": "Y1has9mMvrZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.keras.utils.register_keras_serializable(name='descr')\n",
        "class PreprocLayer(layers.Layer):\n",
        "    def __init__(self, vocab_lookup_layer, **kwargs):\n",
        "        super(PreprocLayer, self).__init__(**kwargs)\n",
        "\n",
        "        # save the constructor parameters for get_config() to work properly\n",
        "        self.vocab_lookup_layer = vocab_lookup_layer\n",
        "\n",
        "    def call(self, descr, training=False):\n",
        "        # split the string on spaces, and make it a rectangular tensor\n",
        "        tokens = tf.strings.split(tf.strings.lower(descr))\n",
        "        tokens = self.vocab_lookup_layer(tokens)\n",
        "        max_len = MAX_LEN # max([x.shape[0] for x in tokens])\n",
        "        input_words = tokens.to_tensor(default_value=0, shape=[tf.rank(tokens), max_len])\n",
        "        return input_words\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        # save constructor args\n",
        "        config['vocab_lookup_layer'] = self.vocab_lookup_layer\n",
        "        return config"
      ],
      "metadata": {
        "id": "D4IAdTxqwJAO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PreprocLayer(vocab_lookup_layer)(['Joe Biden visited Paris'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VEiJBgTxKn6",
        "outputId": "21ad678c-d92a-4d29-e8e8-7a97066a1733"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 16), dtype=int64, numpy=\n",
              "array([[34, 35, 19, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_model = tf.keras.Sequential([\n",
        "  tf.keras.Input(shape=[], dtype=tf.string, name='description'),\n",
        "  PreprocLayer(vocab_lookup_layer),\n",
        "  ner_model                            \n",
        "])\n",
        "temp_model.compile('adam', loss=None)\n",
        "temp_model.save(EXPORT_PATH)\n",
        "!ls -l {EXPORT_PATH}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiUnNPM1xtAS",
        "outputId": "02e46771-debf-4c8b-a7e4-8c26ab426192"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_6_layer_call_fn, embedding_6_layer_call_and_return_conditional_losses, embedding_7_layer_call_fn, embedding_7_layer_call_and_return_conditional_losses, multi_head_attention_3_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ner_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ner_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 716\n",
            "drwxr-xr-x 2 root root   4096 Apr 15 05:19 assets\n",
            "-rw-r--r-- 1 root root  24603 Apr 15 05:51 keras_metadata.pb\n",
            "-rw-r--r-- 1 root root 696087 Apr 15 05:51 saved_model.pb\n",
            "drwxr-xr-x 2 root root   4096 Apr 15 05:51 variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Success!\n",
        "\n",
        "## Prediction\n",
        "\n",
        "Let's try predicting"
      ],
      "metadata": {
        "id": "VV_IV3jjx7Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(EXPORT_PATH)\n",
        "sample_input = [\n",
        "     \"Justin Trudeau went to New Delhi India\",\n",
        "     \"Vladimir Putin was chased out of Kyiv Ukraine\"\n",
        "]\n",
        "model.predict(sample_input)"
      ],
      "metadata": {
        "id": "FpFKfawaSQD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be85d1ba-9928-44b3-bf1e-fd7157fdd211"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[7.6006036e-03, 4.3546227e-03, 9.7820580e-01, 1.3501652e-03,\n",
              "         5.0268644e-03, 3.4619651e-03],\n",
              "        [6.8284925e-03, 1.7240658e-02, 9.1373536e-04, 9.6674633e-01,\n",
              "         5.9596724e-03, 2.3111277e-03],\n",
              "        [2.7760639e-04, 9.9050373e-01, 1.8299931e-03, 2.9289189e-03,\n",
              "         7.0543337e-04, 3.7544277e-03],\n",
              "        [4.2376583e-04, 9.8965186e-01, 1.9375269e-03, 2.5036184e-03,\n",
              "         8.3148218e-04, 4.6517830e-03],\n",
              "        [1.4784709e-02, 1.7444469e-03, 8.8171000e-03, 1.1214550e-02,\n",
              "         9.3378550e-01, 2.9653659e-02],\n",
              "        [4.8861960e-03, 6.6756480e-03, 9.9279033e-03, 5.1787309e-03,\n",
              "         7.7538565e-03, 9.6557772e-01],\n",
              "        [3.6808266e-03, 1.0586737e-02, 9.0764472e-03, 7.3234723e-03,\n",
              "         1.2727647e-02, 9.5660490e-01],\n",
              "        [9.9849033e-01, 4.5291032e-05, 2.8122202e-04, 1.8797908e-04,\n",
              "         3.3309078e-04, 6.6198298e-04],\n",
              "        [9.9861550e-01, 6.5719221e-05, 2.2601534e-04, 2.7777068e-04,\n",
              "         3.2932145e-04, 4.8567922e-04],\n",
              "        [9.9868518e-01, 6.2797822e-05, 2.1537761e-04, 2.9442701e-04,\n",
              "         3.3277631e-04, 4.0954797e-04],\n",
              "        [9.9869895e-01, 5.8080073e-05, 2.2984897e-04, 2.6458810e-04,\n",
              "         3.0653764e-04, 4.4187118e-04],\n",
              "        [9.9878424e-01, 5.1731906e-05, 2.1826439e-04, 2.4978881e-04,\n",
              "         2.9345011e-04, 4.0255187e-04],\n",
              "        [9.9874735e-01, 5.6554803e-05, 2.1676342e-04, 2.7370831e-04,\n",
              "         3.1303157e-04, 3.9254324e-04],\n",
              "        [9.9875116e-01, 5.2218267e-05, 2.4311221e-04, 2.4252495e-04,\n",
              "         2.8749809e-04, 4.2343192e-04],\n",
              "        [9.9878269e-01, 5.2581621e-05, 2.2794459e-04, 2.5742623e-04,\n",
              "         2.9817401e-04, 3.8118821e-04],\n",
              "        [9.9874461e-01, 5.6138193e-05, 2.0102346e-04, 2.7133210e-04,\n",
              "         3.0640230e-04, 4.2041537e-04]],\n",
              "\n",
              "       [[7.3585655e-03, 3.9604446e-03, 9.7752714e-01, 1.3068196e-03,\n",
              "         6.4400239e-03, 3.4069689e-03],\n",
              "        [5.9569501e-03, 1.6661825e-02, 8.2262652e-04, 9.6757543e-01,\n",
              "         6.3061896e-03, 2.6770215e-03],\n",
              "        [2.2904042e-04, 9.9084562e-01, 1.4333606e-03, 3.1978528e-03,\n",
              "         5.3295307e-04, 3.7612703e-03],\n",
              "        [4.7973538e-04, 9.8866910e-01, 1.3205456e-03, 4.1795527e-03,\n",
              "         7.2984997e-04, 4.6213465e-03],\n",
              "        [2.3550792e-04, 9.9037957e-01, 1.7840955e-03, 2.7755927e-03,\n",
              "         6.2943256e-04, 4.1958769e-03],\n",
              "        [2.1252132e-04, 9.8989981e-01, 1.3594244e-03, 3.8911614e-03,\n",
              "         5.5028999e-04, 4.0869247e-03],\n",
              "        [2.2469968e-02, 3.8387438e-03, 1.7444357e-02, 8.5198870e-03,\n",
              "         9.2953789e-01, 1.8189112e-02],\n",
              "        [6.9100210e-03, 4.0428294e-03, 9.1041373e-03, 5.8990256e-03,\n",
              "         1.2094922e-02, 9.6194911e-01],\n",
              "        [9.9860018e-01, 6.6478489e-05, 2.2437646e-04, 2.6501092e-04,\n",
              "         3.3285335e-04, 5.1120349e-04],\n",
              "        [9.9868017e-01, 6.2404797e-05, 2.1390131e-04, 2.8470511e-04,\n",
              "         3.3243810e-04, 4.2642685e-04],\n",
              "        [9.9869859e-01, 5.7365254e-05, 2.2614260e-04, 2.5407012e-04,\n",
              "         3.0472543e-04, 4.5906758e-04],\n",
              "        [9.9878365e-01, 5.1151372e-05, 2.1561405e-04, 2.4051375e-04,\n",
              "         2.9188077e-04, 4.1725705e-04],\n",
              "        [9.9875534e-01, 5.5431345e-05, 2.1146734e-04, 2.5936170e-04,\n",
              "         3.0901056e-04, 4.0944864e-04],\n",
              "        [9.9874336e-01, 5.1851588e-05, 2.4194298e-04, 2.3497983e-04,\n",
              "         2.8732728e-04, 4.4051136e-04],\n",
              "        [9.9879223e-01, 5.1105977e-05, 2.2345623e-04, 2.4223674e-04,\n",
              "         2.9329408e-04, 3.9775384e-04],\n",
              "        [9.9875224e-01, 5.4884214e-05, 1.9733695e-04, 2.5733613e-04,\n",
              "         3.0229447e-04, 4.3596461e-04]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Success!\n",
        "\n",
        "## Postprocessing custom layer\n",
        "\n",
        "But ... now that we know how to do it, let's add a postprocessor of the probabilities ...\n",
        "\n",
        "We can't just do mapping[3] as we would in Python. We have to do it using TensorFlow functions. I won't bore you with the details, this is the relevant code:"
      ],
      "metadata": {
        "id": "fO6A-bclydn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_lookup = tf.lookup.StaticHashTable(\n",
        "    tf.lookup.KeyValueTensorInitializer( \n",
        "        tf.constant(list(mapping.keys())),\n",
        "        tf.constant(list(mapping.values()))),\n",
        "        default_value='[PAD]')\n",
        "mapping_lookup.lookup( tf.constant([0, 1, 2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOxrj6qU39KW",
        "outputId": "8f7f7183-e6d4-48e5-c492-abbcd800f2e9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'[PAD]', b'OUT', b'B-NAME'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.keras.utils.register_keras_serializable(name='tagname')\n",
        "class OutputTagLayer(layers.Layer):\n",
        "    def __init__(self, mapping, **kwargs):\n",
        "        super(OutputTagLayer, self).__init__(**kwargs)\n",
        "\n",
        "        # save the constructor parameters for get_config() to work properly\n",
        "        self.mapping = mapping\n",
        "\n",
        "        # construct\n",
        "        self.mapping_lookup = tf.lookup.StaticHashTable(\n",
        "            tf.lookup.KeyValueTensorInitializer( \n",
        "                tf.range(start=0, limit=len(mapping.values()), delta=1, dtype=tf.int64),\n",
        "                tf.constant(list(mapping.values()))),\n",
        "                default_value='[PAD]')\n",
        "\n",
        "    def call(self, descr_tags, training=False):\n",
        "        prediction = tf.argmax(descr_tags, axis=-1)\n",
        "        prediction = self.mapping_lookup.lookup(prediction)\n",
        "        return prediction\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        # save constructor args\n",
        "        config['mapping'] = self.mapping\n",
        "        return config"
      ],
      "metadata": {
        "id": "I80sAuKaDOg3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OutputTagLayer(mapping)( tf.constant([\n",
        "  [[0.1, 0.2, 0.7], [0.7, 0.2, 0.1], [0.2, 0.7, 0.1]],\n",
        "  [[0.1, 0.2, 0.7], [0.7, 0.2, 0.1], [0.2, 0.7, 0.1]]\n",
        "]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVJbroeHGkFW",
        "outputId": "b728a383-4a54-4446-e344-4cdf9fe2df0c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=string, numpy=\n",
              "array([[b'B-NAME', b'[PAD]', b'OUT'],\n",
              "       [b'B-NAME', b'[PAD]', b'OUT']], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's put the preprocessing and postprocessing layers together:"
      ],
      "metadata": {
        "id": "w-FXQ0QPzJrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_model = tf.keras.Sequential([\n",
        "  tf.keras.Input(shape=[], dtype=tf.string, name='description'),\n",
        "  PreprocLayer(vocab_lookup_layer),\n",
        "  ner_model,\n",
        "  OutputTagLayer(mapping)                             \n",
        "])\n",
        "temp_model.compile('adam', loss=None)\n",
        "temp_model.save(EXPORT_PATH)\n",
        "!ls -l {EXPORT_PATH}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLfqwbSW8KAB",
        "outputId": "ec8c1d4e-d092-44bf-fd39-20f74f942e6f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_6_layer_call_fn, embedding_6_layer_call_and_return_conditional_losses, embedding_7_layer_call_fn, embedding_7_layer_call_and_return_conditional_losses, multi_head_attention_3_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ner_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ner_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 740\n",
            "drwxr-xr-x 2 root root   4096 Apr 15 05:19 assets\n",
            "-rw-r--r-- 1 root root  25295 Apr 15 05:58 keras_metadata.pb\n",
            "-rw-r--r-- 1 root root 716812 Apr 15 05:58 saved_model.pb\n",
            "drwxr-xr-x 2 root root   4096 Apr 15 05:58 variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict using model\n",
        "\n",
        "If you deploy this model, this is the input you will have to send it, and the output you will back from the endpoint (wrapped in a JSON envelope of course)"
      ],
      "metadata": {
        "id": "bWRN0yrANF3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(EXPORT_PATH)\n",
        "sample_input = [\n",
        "     \"Justin Trudeau went to New Delhi India\",\n",
        "     \"Vladimir Putin was chased out of Kyiv Ukraine\"\n",
        "]\n",
        "model.predict(sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065af68a-b64f-4ac6-b0a7-8b0f62848019",
        "id": "pkrGEm34mSSy"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[b'B-NAME', b'I-NAME', b'OUT', b'OUT', b'B-LOCATION',\n",
              "        b'I-LOCATION', b'I-LOCATION', b'[PAD]', b'[PAD]', b'[PAD]',\n",
              "        b'[PAD]', b'[PAD]', b'[PAD]', b'[PAD]', b'[PAD]', b'[PAD]'],\n",
              "       [b'B-NAME', b'I-NAME', b'OUT', b'OUT', b'OUT', b'OUT',\n",
              "        b'B-LOCATION', b'I-LOCATION', b'[PAD]', b'[PAD]', b'[PAD]',\n",
              "        b'[PAD]', b'[PAD]', b'[PAD]', b'[PAD]', b'[PAD]']], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how nice this is ... we send full sentences to the model and we get back the tag for each word in the sentence. Very understandable API!"
      ],
      "metadata": {
        "id": "g0eFP0dCzdu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample inference using the trained model\n",
        "sample_input = [\n",
        "     \"Justin Trudeau went to New Delhi India\",\n",
        "     \"Vladimir Putin was chased out of Kyiv Ukraine\"\n",
        "]\n",
        "predictions = model.predict(sample_input)\n",
        "\n",
        "# print out\n",
        "for idx, descr in enumerate(sample_input):\n",
        "  words = descr.split()\n",
        "  tags = list(predictions[idx])\n",
        "  for word, tag in zip(words, tags):\n",
        "    print(word, '->', tag)\n",
        "  print('-'*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMxLuSLkAzJv",
        "outputId": "96a3a6e8-5c3c-436e-9d05-62732067720f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justin -> b'B-NAME'\n",
            "Trudeau -> b'I-NAME'\n",
            "went -> b'OUT'\n",
            "to -> b'OUT'\n",
            "New -> b'B-LOCATION'\n",
            "Delhi -> b'I-LOCATION'\n",
            "India -> b'I-LOCATION'\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Vladimir -> b'B-NAME'\n",
            "Putin -> b'I-NAME'\n",
            "was -> b'OUT'\n",
            "chased -> b'OUT'\n",
            "out -> b'OUT'\n",
            "of -> b'OUT'\n",
            "Kyiv -> b'B-LOCATION'\n",
            "Ukraine -> b'I-LOCATION'\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2022 Valliappa Lakshmanan\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ],
      "metadata": {
        "id": "yPy_K0VrWfKw"
      }
    }
  ]
}