{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b0fbd2-3a61-4654-83d4-be8fcd576e22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using LLM APIs to extract price information\n",
    "\n",
    "This notebook demonstrates using different LLM APIs to parse pricing\n",
    "information and create a bar chart.\n",
    "\n",
    "Accompanies blog post:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d81390c9-6000-4fc8-a7cd-c1103ff0e07e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U --quiet langchain openai 'google-cloud-aiplatform>=1.38.0' pillow lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e3ee73-2ebb-4fe4-ac1f-cc5693e2ce9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY\n",
      "GOOGLE_API_KEY\n"
     ]
    }
   ],
   "source": [
    "# read config file that has lines of the form\n",
    "# KEY VALUE\n",
    "import os\n",
    "for line in open(\"config.txt\").readlines():\n",
    "    if len(line.strip()) > 0:\n",
    "        key, value = line.split()\n",
    "        print(key)\n",
    "        os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fd2843-0e13-4652-bd8d-3fecf1cdf6ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting apis.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile apis.json\n",
    "[\n",
    "    {\n",
    "        \"id\":   \"Open AI GPT-3.5 Turbo\",\n",
    "        \"name\": \"gpt-3.5-turbo-1106\",\n",
    "        \"price_url\": \"https://openai.com/pricing\",\n",
    "        \"correct_answer\": 0.12\n",
    "    },\n",
    "    {\n",
    "        \"id\":   \"Meta Llama2 on AWS\",\n",
    "        \"name\": \"Llama 2 Chat (70B)\",\n",
    "        \"price_url\": \"https://aws.amazon.com/bedrock/pricing/\",\n",
    "        \"correct_answer\": 0.2206\n",
    "    },\n",
    "    {\n",
    "        \"id\":   \"Google Cloud Gemini Pro\",\n",
    "        \"name\": \"Gemini Pro\",\n",
    "        \"price_url\": \"https://cloud.google.com/vertex-ai/pricing\",\n",
    "        \"correct_answer\": 0.12\n",
    "    },    \n",
    "    {\n",
    "        \"id\":   \"Azure Open AI GPT-3.5 Turbo\",\n",
    "        \"name\": \"GPT-3.5-Turbo-1106\",\n",
    "        \"price_url\": \"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/\",\n",
    "        \"correct_answer\": 0.17\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a30e34a-8761-4aa5-811d-24e6d0078c60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Open AI GPT-3.5 Turbo',\n",
       " 'name': 'gpt-3.5-turbo-1106',\n",
       " 'price_url': 'https://openai.com/pricing',\n",
       " 'correct_answer': 0.12}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "apis = json.load(open(\"apis.json\", \"r\"))\n",
    "apis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fddd830-fdcc-497a-85a4-8aa8e47ac55b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Find text of pricing info within full page\n",
    "\n",
    "The LLMs have limited context, and the full page may not fit into a prompt.\n",
    "So, do a bit of hacking to find the relevant part of the text.\n",
    "In a production application, this might be done by building document embeddings\n",
    "and finding the document chunk(s) that matches the question we are asking.\n",
    "\n",
    "Here, we'll parse out the tables, and find the table that contains the model name.\n",
    "This happens to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185ddfc6-993b-451a-891c-c525d20a5c98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>$0.0010Â / 1K tokens</td>\n",
       "      <td>$0.0020Â / 1K tokens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo-instruct</td>\n",
       "      <td>$0.0015Â / 1K tokens</td>\n",
       "      <td>$0.0020Â / 1K tokens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model                 Input                Output\n",
       "0      gpt-3.5-turbo-1106  $0.0010Â / 1K tokens  $0.0020Â / 1K tokens\n",
       "1  gpt-3.5-turbo-instruct  $0.0015Â / 1K tokens  $0.0020Â / 1K tokens"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "    \n",
    "def get_price_info(api):\n",
    "    full_page = requests.get(api['price_url']).text\n",
    "    soup = BeautifulSoup(full_page, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    for table in tables:\n",
    "        spans = table.find_all('span')\n",
    "        for span in spans:\n",
    "            # msft ...\n",
    "            if span.has_attr('data-amount'):\n",
    "                spandata = span['data-amount']\n",
    "                price = json.loads(spandata)['regional'].get('us-east', 0)\n",
    "                sup = soup.new_tag('sup')\n",
    "                sup.string = str(price)\n",
    "                span.insert_after(sup)\n",
    "                span.delete\n",
    "            \n",
    "        tbl_str = str(table)\n",
    "        if tbl_str.find(api['name']) > 0:\n",
    "            df = pd.read_html(tbl_str, header=0)[0]\n",
    "            return df\n",
    "        \n",
    "    return \"\"\n",
    "\n",
    "get_price_info(apis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4430fea6-f9a8-4de9-9113-8dc5aecadde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      " Open AI GPT-3.5 Turbo \n",
      " ,Model,Input,Output\n",
      "0,gpt-3.5-turbo-1106,$0.0010Â / 1K tokens,$0.0020Â / 1K tokens\n",
      "1,gpt-3.5-turbo-instruct,$0.0015Â / 1K tokens,$0.0020Â / 1K tokens\n",
      " \n",
      "\n",
      "***\n",
      " Meta Llama2 on AWS \n",
      " ,Meta models,\"Price per 1,000 input tokens\",\"Price per 1,000 output tokens\"\n",
      "0,Llama 2 Chat (13B),$0.00075,$0.00100\n",
      "1,Llama 2 Chat (70B),$0.00195,$0.00256\n",
      " \n",
      "\n",
      "***\n",
      " Google Cloud Gemini Pro \n",
      " ,Model,Feature,Type,Price\n",
      "0,Gemini Pro,Multimodal,Image Input Video Input Text Input,$0.0025 / image $0.002 / second $0.00025 / 1k characters\n",
      "1,Gemini Pro,,Text Output,$0.0005 / 1k characters\n",
      " \n",
      "\n",
      "***\n",
      " Azure Open AI GPT-3.5 Turbo \n",
      " ,Models,Context,\"Prompt (Per 1,000 tokens)\",\"Completion (Per 1,000 tokens)\"\n",
      "0,GPT-3.5-Turbo,4K,$-0.0015,$-0.002\n",
      "1,GPT-3.5-Turbo,16K,$-0.003,$-0.004\n",
      "2,GPT-3.5-Turbo-1106,16K,$-0,$-0\n",
      "3,GPT-4-Turbo,128K,$-0,$-0\n",
      "4,GPT-4-Turbo-Vision,128K,$-0,$-0\n",
      "5,GPT-4,8K,$-0.03,$-0.06\n",
      "6,GPT-4,32K,$-0.06,$-0.12\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for api in apis:\n",
    "    price_info = get_price_info(api).to_csv()\n",
    "    print(\"***\\n\",api['id'],\"\\n\", price_info,\"\\n\")\n",
    "    api['price_info'] = price_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90ef1c2e-845b-4428-86e1-69ffc317070f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Using the pricing info and example below, find the cost of sending\n",
      "    1000 tokens of text to xyz-api-3 and receiving 100 tokens back.  \n",
      "                                    \n",
      "                                    **Price Info**\n",
      "                                    \n",
      ",Model,Input,Output\n",
      "0,xyz-api-3,$0.05 / 1K tokens,$0.02 / 1K tokens\n",
      "0,abc-turbo-5,$0.08 / 1K tokens,$0.004 / 1K tokens \n",
      "               \n",
      "    \n",
      "  \n",
      "                                    **Answer**\n",
      "                                    \n",
      "    Because price of xyz-api-3 for 1k tokens input is $0.05 and price for 1k tokens output is $0.02,\n",
      "    the price for 1000 tokens of input + 100 tokens of output = (1000/1000)*$0.05 + (100/1000)*$0.02 = $0.052            \n",
      "            \n",
      "                                    \n",
      "\n",
      "\n",
      "    Using the pricing info and example below, find the cost of sending\n",
      "    1000 tokens of text to abc-turbo-5 and receiving 100 tokens back.  \n",
      "                                    \n",
      "                                    **Price Info**\n",
      "                                    \n",
      ",Model,Prompt (per 1k characters),Output (per 1k characters)\n",
      "0,abc-turbo-5,$0.08,$0.004  \n",
      "0,xyz-api-3,$0.05,$0.02\n",
      "               \n",
      "    \n",
      "  \n",
      "                                    **Answer**\n",
      "                                    \n",
      "    Because price of abc-turbo-5 for 1k characters input is $0.08 and price for 1k characters output is $0.004,\n",
      "    the price for 1k tokens input is $0.08*4=$0.12 and price for 1k tokens output is $0.004*4=$0.016\n",
      "    the price for 1000 tokens of input + 100 tokens of output = (1000/1000)*$0.12 + (100/1000)*$0.016 = $0.1216            \n",
      "            \n",
      "                                    \n",
      "\n",
      "\n",
      "    Using the pricing info and example below, find the cost of sending\n",
      "    1000 tokens of text to gpt-3.5-turbo-1106 and receiving 100 tokens back.  \n",
      "                                    \n",
      "                                    **Price Info**\n",
      "                                    ,Model,Input,Output\n",
      "0,gpt-3.5-turbo-1106,$0.0010Â / 1K tokens,$0.0020Â / 1K tokens\n",
      "1,gpt-3.5-turbo-instruct,$0.0015Â / 1K tokens,$0.0020Â / 1K tokens\n",
      "   \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "def create_fewshot_prompt():  \n",
    "    examples = [\n",
    "        {\n",
    "            \"price_info\": \"\"\"\n",
    ",Model,Input,Output\n",
    "0,xyz-api-3,$0.05 / 1K tokens,$0.02 / 1K tokens\n",
    "0,abc-turbo-5,$0.08 / 1K tokens,$0.004 / 1K tokens \n",
    "            \"\"\",\n",
    "            \"name\": \"xyz-api-3\",\n",
    "            \"answer\": \"\"\"\n",
    "    Because price of xyz-api-3 for 1k tokens input is $0.05 and price for 1k tokens output is $0.02,\n",
    "    the price for 1000 tokens of input + 100 tokens of output = (1000/1000)*$0.05 + (100/1000)*$0.02 = $0.052            \n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"price_info\": \"\"\"\n",
    ",Model,Prompt (per 1k characters),Output (per 1k characters)\n",
    "0,abc-turbo-5,$0.08,$0.004  \n",
    "0,xyz-api-3,$0.05,$0.02\n",
    "            \"\"\",\n",
    "            \"name\": \"abc-turbo-5\",\n",
    "            \"answer\": \"\"\"\n",
    "    Because price of abc-turbo-5 for 1k characters input is $0.08 and price for 1k characters output is $0.004,\n",
    "    the price for 1k tokens input is $0.08*4=$0.12 and price for 1k tokens output is $0.004*4=$0.016\n",
    "    the price for 1000 tokens of input + 100 tokens of output = (1000/1000)*$0.12 + (100/1000)*$0.016 = $0.1216            \n",
    "            \"\"\"\n",
    "        },        \n",
    "    ]\n",
    "    \n",
    "    question_string = \"\"\"\n",
    "    Using the pricing info and example below, find the cost of sending\n",
    "    1000 tokens of text to {name} and receiving 100 tokens back.  \n",
    "                                    \n",
    "                                    **Price Info**\n",
    "                                    {price_info}   \n",
    "    \"\"\"\n",
    "    \n",
    "    example_prompt = PromptTemplate(input_variables=[\"name\", \"price_info\", \"answer\"],\n",
    "                                    template=question_string + \"\"\"\n",
    "  \n",
    "                                    **Answer**\n",
    "                                    {answer}\n",
    "                                    \"\"\")\n",
    "\n",
    "    # print(example_prompt.format(**examples[0]))\n",
    "    \n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        suffix=question_string,\n",
    "        input_variables=[\"name\", \"price_info\"]\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "prompt = create_fewshot_prompt()\n",
    "print(prompt.format(name=apis[0]['name'], price_info=apis[0]['price_info']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c7c5ce8-4859-4ea9-ad58-f9d188e4beff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fewshot_prompt = create_fewshot_prompt()\n",
    "\n",
    "def get_price(api, llm):\n",
    "    result = llm(fewshot_prompt.format(name=api['name'], price_info=api['price_info']))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27ab2d8d-fc81-4c4a-9056-995c8607123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_dataframe(apis):\n",
    "    return pd.DataFrame.from_dict({\n",
    "        'id': [api['id'] for api in apis],\n",
    "        'estimated_price': [float(api['estimated_price'].replace('$','')) for api in apis],\n",
    "        'correct_answer': [float(api['correct_answer']) for api in apis]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c192609-0ec6-4442-8606-32de91b01b80",
   "metadata": {},
   "source": [
    "## Open AI GPT 3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b80c726c-9438-4d72-849d-8c138b67b1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      " Open AI GPT-3.5 Turbo \n",
      " \n",
      "  \n",
      "                                    **Answer**\n",
      "                                    \n",
      "    Because price of gpt-3.5-turbo-1106 for 1k tokens input is $0.0010 and price for 1k tokens output is $0.0020,\n",
      "    the price for 1000 tokens of input + 100 tokens of output = (1000/1000)*$0.0010 + (100/1000)*$0.0020 = $0.0030            \n",
      "            \n",
      "                                     \n",
      "\n",
      "***\n",
      " Meta Llama2 on AWS \n",
      " \n",
      "  \n",
      "                                    **Answer**\n",
      "                                    \n",
      "    Because price of Llama 2 Chat (70B) for 1k tokens input is $0.00195 and price for 1k tokens output is $0.00256,\n",
      "    the price for 1000 tokens of input + 100 tokens of output = (1000/1000)*$0.00195 + (100/1000)*$0.00256 = $0.00451            \n",
      "            \n",
      "                                     \n",
      "\n",
      "***\n",
      " Google Cloud Gemini Pro \n",
      " \n",
      "  \n",
      "                                    **Answer**\n",
      "                                    \n",
      "    Because price of Gemini Pro for 1k characters input is $0.00025 and price for 1k characters output is $0.0005,\n",
      "    the price for 1000 tokens of input + 100 tokens of output = (1000/1000)*$0.00025 + (100/1000)*$0.0005 = $0.0035            \n",
      "            \n",
      "                                     \n",
      "\n",
      "***\n",
      " Azure Open AI GPT-3.5 Turbo \n",
      " \n",
      "  \n",
      "                                    **Answer**\n",
      "                                    \n",
      "    Because price of GPT-3.5-Turbo-1106 for 1k tokens input is $0 and price for 1k tokens output is $0,\n",
      "    the price for 1000 tokens of input + 100 tokens of output = (1000/1000)*$0 + (100/1000)*$0 = $0            \n",
      "            \n",
      "                                     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "for api in apis:\n",
    "    price_response = get_price(api, llm)\n",
    "    print(\"***\\n\", api['id'], \"\\n\", price_response, \"\\n\")\n",
    "    api['estimated_price'] = price_response.split()[-1] # last word\n",
    "    \n",
    "    import time\n",
    "    time.sleep(30) # openai token limits ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1452b13-8a6e-4030-920d-3685be64e841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>estimated_price</th>\n",
       "      <th>correct_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Open AI GPT-3.5 Turbo</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta Llama2 on AWS</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>0.2206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Cloud Gemini Pro</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Azure Open AI GPT-3.5 Turbo</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>0.1700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  estimated_price  correct_answer\n",
       "0        Open AI GPT-3.5 Turbo           0.1200          0.1200\n",
       "1           Meta Llama2 on AWS           0.2950          0.2206\n",
       "2      Google Cloud Gemini Pro           0.3000          0.1200\n",
       "3  Azure Open AI GPT-3.5 Turbo          -0.0055          0.1700"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tbl = make_dataframe(apis)\n",
    "results_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00ce75-c79d-42b2-ac67-eeb2fa4f8888",
   "metadata": {},
   "source": [
    "It's hit-and-miss.\n",
    "\n",
    "This illustrates how far you have to go to get deterministic, accurate answers out of an LLM-based software service.  Especially if it involves math of any kind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0e0ef-c466-4743-9e8f-736382756915",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gemini Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "251bbc44-5f5b-47ee-b1eb-4e05fb17f451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Content has no parts.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m llm \u001b[38;5;241m=\u001b[39m VertexAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m api \u001b[38;5;129;01min\u001b[39;00m apis:\n\u001b[0;32m----> 5\u001b[0m     price_response \u001b[38;5;241m=\u001b[39m \u001b[43mget_price\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m***\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, api[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, price_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     api[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimated_price\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m price_response\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# last word\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m, in \u001b[0;36mget_price\u001b[0;34m(api, llm)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_price\u001b[39m(api, llm):\n\u001b[0;32m----> 4\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfewshot_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprice_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice_info\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:892\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 892\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    902\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:666\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    652\u001b[0m         )\n\u001b[1;32m    653\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    654\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    655\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m         )\n\u001b[1;32m    665\u001b[0m     ]\n\u001b[0;32m--> 666\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:553\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    552\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    554\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py:540\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    532\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 540\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    544\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    549\u001b[0m         )\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/llms/vertexai.py:336\u001b[0m, in \u001b[0;36mVertexAI._generate\u001b[0;34m(self, prompts, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m         res \u001b[38;5;241m=\u001b[39m completion_with_retry(\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    329\u001b[0m             [prompt],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    334\u001b[0m         )\n\u001b[1;32m    335\u001b[0m         generations\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 336\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_to_generation(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mcandidates]\n\u001b[1;32m    337\u001b[0m         )\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/llms/vertexai.py:336\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m         res \u001b[38;5;241m=\u001b[39m completion_with_retry(\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    329\u001b[0m             [prompt],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    334\u001b[0m         )\n\u001b[1;32m    335\u001b[0m         generations\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 336\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_to_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mcandidates]\n\u001b[1;32m    337\u001b[0m         )\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/llms/vertexai.py:305\u001b[0m, in \u001b[0;36mVertexAI._response_to_generation\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GenerationChunk(text\u001b[38;5;241m=\u001b[39m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m, generation_info\u001b[38;5;241m=\u001b[39mgeneration_info)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:1366\u001b[0m, in \u001b[0;36mCandidate.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m-> 1366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:1423\u001b[0m, in \u001b[0;36mContent.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiple content parts are not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts:\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent has no parts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mValueError\u001b[0m: Content has no parts."
     ]
    }
   ],
   "source": [
    "from langchain.llms import VertexAI\n",
    "llm = VertexAI(model_name=\"gemini-pro\")\n",
    "\n",
    "for api in apis:\n",
    "    price_response = get_price(api, llm)\n",
    "    print(\"***\\n\", api['id'], \"\\n\", price_response, \"\\n\")\n",
    "    api['estimated_price'] = price_response.split()[-1] # last word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d397f0c-a229-40ec-8f63-039278373ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dataframe(apis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0e78fb6-0349-4d16-9bfe-84b38b756ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"w-full border-t border-t-primary xs:hidden md:table md:w-[calc(100%+var(--inner-gutter))]\"><tbody><!--[--><!--[--><tr class=\"border-b border-secondary\"><!--[--><td class=\"pt-8 pb-8 xs:w-auto md:w-1/2 lg:w-1/3\"><span class=\"f-heading-5\">Model</span><!-- --></td><td class=\"pt-8 pb-8\"><span class=\"f-heading-5\">Input</span><!-- --></td><td class=\"pt-8 pb-8\"><span class=\"f-heading-5\">Output</span><!-- --></td><!--]--></tr><!--]--><!--[--><tr class=\"border-b border-secondary\"><!--[--><td class=\"pt-8 pb-8\"><span class=\"f-body-1\">gpt-4-1106-preview</span><!-- --></td><td class=\"pt-8 pb-8\"><span class=\"f-body-1\">$0.01</span><span class=\"f-body-1 text-secondary\">Â / 1K tokens</span></td><td class=\"pt-8 pb-8\"><span class=\"f-body-1\">$0.03</span><span class=\"f-body-1 text-secondary\">Â / 1K tokens</span></td><!--]--></tr><!--]--><!--[--><tr class=\"border-b border-secondary\"><!--[--><td class=\"pt-8 pb-8\"><span class=\"f-body-1\">gpt-4-1106-vision-preview</span><!-- --></td><td class=\"pt-8 pb-8\"><span class=\"f-body-1\">$0.01</span><span class=\"f-body-1 text-secondary\">Â / 1K tokens</span></td><td class=\"pt-8 pb-8\"><span class=\"f-body-1\">$0.03</span><span class=\"f-body-1 text-secondary\">Â / 1K tokens</span></td><!--]--></tr><!--]--><!--]--></tbody></table>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "get_price_info(apis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8dd86e-aad8-48c0-a9af-f672d5877a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
